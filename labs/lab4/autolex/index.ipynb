{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Автолекс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CFG import CFG\n",
    "from RG import RG\n",
    "from Rule import Rule\n",
    "from Token import Token\n",
    "from PumpTree import PumpTree\n",
    "from Test import Test\n",
    "\n",
    "from utils import add_method, Set\n",
    "import os\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обертка в охранные нетерминалы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guard(cfg):\n",
    "    previous_cfg = copy(cfg)\n",
    "    regular_nterms = previous_cfg.regular_nterms()\n",
    "    new_rules = []\n",
    "    for nterm in previous_cfg.nterms:\n",
    "        if nterm in regular_nterms:\n",
    "            continue\n",
    "\n",
    "        for rule in previous_cfg[nterm]:\n",
    "            for token in rule.right:\n",
    "                if token.is_term():\n",
    "                    left = Token(token.value)\n",
    "                    left.is_guard = True\n",
    "                    new_rule = Rule(left, [Token(token.value)])\n",
    "                    if new_rule not in new_rules:\n",
    "                        new_rules.append(new_rule)\n",
    "                    token.is_guard = True\n",
    "    \n",
    "    new_rules += previous_cfg.rules\n",
    "    \n",
    "    return CFG(previous_cfg.start_nterm, new_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIRST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_method(CFG)\n",
    "def FIRST(self, tokens, k=1):\n",
    "    first = {nterm : Set() for nterm in self.nterms}\n",
    "    \n",
    "    while True:\n",
    "        previous_first = first.copy()\n",
    "        for nterm in first:\n",
    "            for rule in self[nterm]:\n",
    "                product = Set([''])\n",
    "                for token in rule.right:\n",
    "                    if token.is_term():\n",
    "                        product *= Set([str(token)])\n",
    "                    else:\n",
    "                        product *= first[token]\n",
    "                first[nterm] += product.first(k)\n",
    "        \n",
    "        if previous_first == first:\n",
    "            break\n",
    "    \n",
    "    product = Set([''])\n",
    "    for token in tokens:\n",
    "        if token.is_term():\n",
    "            product *= Set([str(token)])\n",
    "        else:\n",
    "            product *= first[token]\n",
    "    \n",
    "    return product.first(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FOLLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_method(CFG)\n",
    "def FOLLOW(self, nterm, k=1):\n",
    "    follow = {nterm : Set() for nterm in self.nterms}\n",
    "    follow[self.start_nterm] = Set(['$'])\n",
    "    while True:\n",
    "        previous_follow = follow.copy()\n",
    "        for rule in self.rules:\n",
    "            if rule.is_terminal():\n",
    "                continue\n",
    "            for partition in rule.partitions():\n",
    "                follow[partition[0]] += Set.first(\n",
    "                    self.FIRST(partition[1][1], k) * follow[rule.left],\n",
    "                    k\n",
    "                )\n",
    "        if previous_follow == follow:\n",
    "            break\n",
    "    \n",
    "    return follow[nterm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_method(CFG)\n",
    "def LAST(self, tokens, k=1):\n",
    "    last = {nterm : Set() for nterm in self.nterms}\n",
    "    \n",
    "    while True:\n",
    "        previous_last = last.copy()\n",
    "        for nterm in last:\n",
    "            for rule in self[nterm]:\n",
    "                product = Set([''])\n",
    "                for token in rule.right:\n",
    "                    if token.is_term():\n",
    "                        product *= Set([str(token)])\n",
    "                    else:\n",
    "                        product *= last[token]\n",
    "                last[nterm] += product.last(k)\n",
    "        \n",
    "        if previous_last == last:\n",
    "            break\n",
    "    \n",
    "    product = Set([''])\n",
    "    for token in tokens:\n",
    "        if token.is_term():\n",
    "            product *= Set([str(token)])\n",
    "        else:\n",
    "            product *= last[token]\n",
    "    \n",
    "    return product.last(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRECEDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_method(CFG)\n",
    "def PRECEDE(self, nterm, k=1):\n",
    "    precede = {nterm : Set() for nterm in self.nterms}\n",
    "    precede[self.start_nterm] = Set(['^'])\n",
    "    while True:\n",
    "        previous_precede = precede.copy()\n",
    "        for rule in self.rules:\n",
    "            if rule.is_terminal():\n",
    "                continue\n",
    "            for partition in rule.partitions():\n",
    "                precede[partition[0]] += Set.last(\n",
    "                    precede[rule.left] * self.LAST(partition[1][0], k),\n",
    "                    k\n",
    "                )\n",
    "        if previous_precede == precede:\n",
    "            break\n",
    "    \n",
    "    return precede[nterm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поиск токенов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Утверждение** \\\n",
    "Терминал $a$ принадлежит языку нетерминала $T$ $\\Leftrightarrow$ $a \\in {FIRST}_{2}(T)$\n",
    "\n",
    "**Следствие** \\\n",
    "Пусть $T$ - регулярный нетерминал. \\\n",
    "$T$ является токеном $\\Leftrightarrow {FIRST}_{2}(T) \\cap ({FOLLOW}_{1}(T) \\cup {PRECEDE}_{1}(T)) = \\varnothing$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(cfg):\n",
    "    tokens = set()\n",
    "    conflicted = set()\n",
    "    for nterm in cfg.regular_nterms():\n",
    "        if len(\n",
    "            cfg.FIRST([nterm], 2)\n",
    "            .intersection(\n",
    "                cfg.FOLLOW(nterm, 1) + \n",
    "                cfg.PRECEDE(nterm, 1)\n",
    "            )\n",
    "        ) == 0:\n",
    "            tokens.add(nterm)\n",
    "        else:\n",
    "            conflicted.add(nterm)\n",
    "    \n",
    "    return tokens, conflicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Порождение регулярных выражений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Под *заведомо регулярным* нетерминалом будем понимать регулярный нетерминал, найденный до замыкания.\n",
    "\n",
    "Пусть $G = \\langle \\Sigma, N, S, P \\rangle$ - исходная грамматика, $M \\subset N$ - множество регулярных нетерминалов, $M_R \\subset M$ - множество заведомо регулярных нетерминалов,  $M_T \\subset M$ - множество токенов.\n",
    "\n",
    "**Алгоритм**\n",
    "1. Для всех $A \\in M_R$ строим грамматику $G_A = \\langle \\Sigma, N, A, P \\rangle$\n",
    "2. Фильтруем полезные нетерминалы $G_A$\n",
    "3. Если $G_A$ - леволинейная грамматика, переводим ее в праволинейную\n",
    "4. Для $A$ порождаем регулярное выражение по $G_A$\n",
    "5. Для всех $B \\in M \\setminus M_R$ строим дерево разбора $T$ до первых накачек $\\gamma$, таких, что $\\exists C \\in N: (C \\in \\gamma \\Rightarrow C \\in M')$\n",
    "6. Порождаем регулярное выражение для $B = \\displaystyle \\sum_{\\alpha \\in leaves(T)} substitute(\\alpha, M') $\n",
    "7. Для всех $C \\in M_T$ возвращаем регулярное выражение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_regexes(cfg, tokens):\n",
    "\tlr_nterms = cfg.regular_subset(Rule.is_left_linear)\n",
    "\n",
    "\trr_nterms = cfg.regular_subset(Rule.is_right_linear)\n",
    "\t\n",
    "\tother_nterms = cfg.regular_nterms().difference(lr_nterms.union(rr_nterms))\n",
    "\n",
    "\tregexes = {}\n",
    "\tfor nterm in lr_nterms:\n",
    "\t\trg = RG.from_cfg(cfg, nterm)\n",
    "\t\trg.to_right_linear()\n",
    "\t\tregexes[nterm] = rg.to_regex()\n",
    "\tfor nterm in rr_nterms:\n",
    "\t\trg = RG.from_cfg(cfg, nterm)\n",
    "\t\tregexes[nterm] = rg.to_regex()\n",
    "\tfor nterm in other_nterms:\n",
    "\t\ttree = PumpTree(cfg, nterm, regexes)\n",
    "\t\tregexes[nterm] = tree.generate_regex()\n",
    "\t\n",
    "\tresult = {}\n",
    "\tfor nterm in regexes:\n",
    "\t\tif nterm in tokens:\n",
    "\t\t\tresult[nterm] = regexes[nterm]\n",
    "\t\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Автолексер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autolex(cfg):\n",
    "    guarded_cfg = guard(cfg)\n",
    "    tokens, conflicted = tokenize(guarded_cfg)\n",
    "    regexes = token_regexes(guarded_cfg, tokens)\n",
    "\n",
    "    return guarded_cfg, regexes, conflicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тесты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec36dabe01fd4a4ab3a9ade88be36ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(HTML(value='\\n\\t\\t\\t<div>\\n\\t\\t\\t\\t<p>\\n\\t\\t\\t\\t\\t<b>КС-грамматика</b><br>\\n\\t\\t\\t\\t\\t[S] ::= a[…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TESTS_DIR = 'tests'\n",
    "\n",
    "for test_name in os.listdir(TESTS_DIR):\n",
    "    test = Test(\n",
    "        test_name.split('.')[0],\n",
    "        os.path.join(TESTS_DIR, test_name)\n",
    "    )\n",
    "    test.execute(autolex)\n",
    "    Test.add(test)\n",
    "\n",
    "Test.display_results()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
